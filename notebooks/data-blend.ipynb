{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from src import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_clean_tokens(firmname):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    full_punctuation = string.punctuation + 'â€™'\n",
    "    translator = str.maketrans(full_punctuation, ' '*len(full_punctuation))\n",
    "    \n",
    "    decrease = 1\n",
    "    while decrease > 0:\n",
    "        start_len = len(firmname)\n",
    "        firmname = firmname.replace('  ', ' ')\n",
    "        decrease = start_len - len(firmname)\n",
    "    tokens = firmname.translate(translator).split()\n",
    "    tokens_cleans = [re.sub(\"[^0-9a-zA-Z]+\", \"\", t).lower() for t in tokens]\n",
    "    tokens_filtered = list(set([t for t in tokens_cleans if len(t) > 0]))#.sort(reverse=True)\n",
    "    \n",
    "    return tokens_filtered\n",
    "\n",
    "\n",
    "def sequence_uniqueness(tokens, token_frequency_dict):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    return sum(1 / token_frequency_dict[t] ** 0.5 for t in tokens)\n",
    "\n",
    "\n",
    "def name_similarity(name_a, name_b, token_frequency):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    a_tokens = set(name_a)\n",
    "    b_tokens = set(name_b)\n",
    "    a_uniq = sequence_uniqueness(name_a, token_frequency)\n",
    "    b_uniq = sequence_uniqueness(name_b, token_frequency)\n",
    "    if a_uniq == 0 or b_uniq == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return sequence_uniqueness(a_tokens.intersection(b_tokens), token_frequency) / (a_uniq * b_uniq) ** 0.5\n",
    "\n",
    "    \n",
    "def build_token_frequency_table(token_lists):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    tokens = [str(token) for s in token_lists for token in s]\n",
    "    return Counter(tokens)\n",
    "\n",
    "def column_to_date(dataf, column):\n",
    "    \n",
    "    raw_dates = dataf[column].values\n",
    "\n",
    "    new_dates = []\n",
    "    for rd in raw_dates:\n",
    "        succeed = False\n",
    "        try:\n",
    "            nd = datetime.strptime(rd, '%d/%m/%Y')\n",
    "            succeed = True\n",
    "        except:\n",
    "            pass\n",
    "        if not succeed:\n",
    "            try:\n",
    "                nd = datetime.strptime(f'01/01/{rd}', '%d/%m/%Y')\n",
    "                succeed = True\n",
    "            except:\n",
    "                pass\n",
    "        if succeed:\n",
    "            new_dates.append(nd.strftime('%Y-%m-%d'))\n",
    "        else:\n",
    "            new_dates.append(None)\n",
    "\n",
    "    dataf[column] = new_dates\n",
    "    \n",
    "    return dataf\n",
    "\n",
    "def match_firm_hash(dfbase, dfmatch, min_score=0.5):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    base_firm_names = list(dfbase['firm'].values)\n",
    "    base_firm_hashed = list(dfbase['firmhash'].values)\n",
    "\n",
    "    firm_hash_dict = {}\n",
    "    for bn, bh in zip(base_firm_names, base_firm_hashed):\n",
    "        firm_hash_dict[bn] = {\n",
    "            'hash': bh,\n",
    "            'tokens': to_clean_tokens(bn)\n",
    "        }\n",
    "\n",
    "    match_firm_names = list(dfmatch['firm'].values)\n",
    "    match_firms_tokenized = [to_clean_tokens(f) for f in match_firm_names]\n",
    "    match_firm_dict = {}\n",
    "    for mn, mt in zip(match_firm_names, match_firms_tokenized):\n",
    "        match_firm_dict[mn] = {\n",
    "            'tokens': mt,\n",
    "        }\n",
    "\n",
    "    all_companies_tokenized = [*[v['tokens'] for v in firm_hash_dict.values()], *match_firms_tokenized]\n",
    "\n",
    "    token_frequency = build_token_frequency_table(all_companies_tokenized)\n",
    "    \n",
    "    no_match = 0\n",
    "    for mk, mv in match_firm_dict.items():\n",
    "        match_scores = []\n",
    "        max_score = 0\n",
    "        best_match = None\n",
    "        for bk, bv in firm_hash_dict.items():\n",
    "            try:\n",
    "                subscore = name_similarity(bv['tokens'], mv['tokens'], token_frequency)\n",
    "            except:\n",
    "                print('Error on similarity match')\n",
    "                print(bv['tokens'])\n",
    "                print(mk)\n",
    "            if subscore > min_score:\n",
    "                if subscore > max_score:\n",
    "                    max_score = subscore\n",
    "                    best_match = bv['hash']\n",
    "        match_firm_dict[mk]['hash'] = best_match\n",
    "        match_firm_dict[mk]['match_score'] = max_score\n",
    "        if max_score <= min_score:\n",
    "            no_match += 1\n",
    "            print(f'No match found for: {mk}')\n",
    "\n",
    "    df_hashmatch = pd.DataFrame({\n",
    "        'firm': match_firm_dict.keys(),\n",
    "        'firmhash': [v['hash'] for v in match_firm_dict.values()]\n",
    "    })\n",
    "    df_hashmatch['firmhash'] = df_hashmatch['firmhash'].fillna(0).astype(int)\n",
    "\n",
    "    dfmatch = dfmatch.merge(df_hashmatch, on='firm', how='left')\n",
    "\n",
    "    return dfmatch.drop(columns=['firm']), no_match, token_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "dffortune = pd.read_csv('../data/fortune/f500_full_firm_data.csv')\n",
    "dffortune_sample = dffortune.loc[lambda x: (x['ranking']<=300) & (x['include']==True)].drop(columns=['include', 'ranklabel']).reset_index(drop=True)\n",
    "\n",
    "dffortune_sample_r_growth = dffortune_sample.loc[lambda x: (~x['reputation_score_2020'].isnull()) & (~x['reputation_score_2019'].isnull())][['reputation_score_2020', 'reputation_score_2019']]\n",
    "dffortune_sample_r_growth = dffortune_sample_r_growth.loc[lambda x: (x['reputation_score_2020']!='-') & (x['reputation_score_2019']!='-')]\n",
    "\n",
    "dffortune_sample_r_growth = dffortune_sample_r_growth.astype(float).round(5)\n",
    "dffortune_sample_r_growth['reputation_score_growth'] = (dffortune_sample_r_growth['reputation_score_2020'] - dffortune_sample_r_growth['reputation_score_2019']) / dffortune_sample_r_growth['reputation_score_2019']\n",
    "\n",
    "dffortune_sample = pd.concat([dffortune_sample, dffortune_sample_r_growth[['reputation_score_growth']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp_features = pd.read_csv('../data/policies/features/firm_pp_features_0.2.0.csv')\n",
    "df_pp_features = df_pp_features.drop(columns=['firm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prc_data_breaches = pd.read_csv('../data/breaches/prc_firm_data_breach_matches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_control_1 = pd.read_csv('../data/dbs/date_of_incorporation_and_stock_turnover.csv')\n",
    "\n",
    "df_control_1['stock_turnover'] = df_control_1['stock_turnover'].str.replace(',', '.')\n",
    "df_control_1['stock_turnover'] = df_control_1['stock_turnover'].replace('n.a.', None)\n",
    "df_control_1['stock_turnover'] = df_control_1['stock_turnover'].replace('n.s.', None)\n",
    "df_control_1['stock_turnover'] = df_control_1['stock_turnover'].astype(float)\n",
    "\n",
    "df_control_1 = column_to_date(df_control_1, 'date_of_incorporation')\n",
    "\n",
    "df_control_1['age_in_years'] = round((pd.Timestamp.now() - pd.to_datetime(df_control_1['date_of_incorporation'])).dt.days / 364.24, 1)\n",
    "\n",
    "df_control_1, _, __ = match_firm_hash(dffortune_sample, df_control_1)\n",
    "print(f'n no match: {_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-indie",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contron_2 = pd.read_csv('../data/dbs/return_on_assets.csv')\n",
    "df_contron_2 = df_contron_2[['firm', 'return_on_assets']].copy()\n",
    "df_contron_2['return_on_assets'] = df_contron_2['return_on_assets'].str.replace(',', '.').astype(float)\n",
    "df_contron_2, _, __ = match_firm_hash(dffortune_sample, df_contron_2)\n",
    "print(f'n no match: {_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employee = pd.read_csv('../data/dbs/employee_satisfaction_glassdoor.csv')\n",
    "df_employee = df_employee.drop(columns=['Industry', 'Sector'])\n",
    "df_employee.columns = ['_'.join(to_clean_tokens(c)) for c in df_employee.columns]\n",
    "df_employee, _, __ = match_firm_hash(dffortune_sample, df_employee)\n",
    "print(f'n no match: {_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ig_manuals = pd.read_csv('../data/dbs/information_governance_practises_manuals.csv')\n",
    "df_ig_manuals['privacy_policy_url'] = df_ig_manuals['Privacy Policy URL corrected']\n",
    "df_ig_manuals['privacy_policy_url'] = df_ig_manuals['privacy_policy_url'].fillna(df_ig_manuals['Privacy Policy ULR'])\n",
    "df_ig_manuals = df_ig_manuals.drop(columns=['Industry', 'Sector', 'Collector ', 'Comment', 'Privacy Policy ULR', 'Privacy Policy URL corrected'])\n",
    "df_ig_manuals.columns = ['_'.join(to_clean_tokens(c)) for c in df_ig_manuals.columns]\n",
    "df_ig_manuals, _, __ = match_firm_hash(dffortune_sample, df_ig_manuals)\n",
    "print(f'n no match: {_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iss_share_pros = pd.read_csv('../data/dbs/iss_shareholder_proposals.csv')\n",
    "df_iss_share_pros.columns = ['_'.join(to_clean_tokens(c)) for c in df_iss_share_pros.columns]\n",
    "df_iss_share_pros = column_to_date(df_iss_share_pros, 'date_meeting')\n",
    "df_iss_share_pros = df_iss_share_pros.rename(columns={'company_name': 'firm'})\n",
    "\n",
    "df_iss_share_pros_count = df_iss_share_pros.groupby('firm', as_index=False)[['other_status']].count().rename(columns={'other_status': 'number_of_shareholder_proposals'})\n",
    "\n",
    "df_iss_share_pros_count, _, __ = match_firm_hash(dffortune_sample, df_iss_share_pros_count)\n",
    "print(f'n no match: {_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-rwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blended = dffortune_sample \\\n",
    ".merge(df_pp_features, how='left', on='firmhash') \\\n",
    ".merge(df_prc_data_breaches, how='left', on='firmhash') \\\n",
    ".merge(df_control_1, how='left', on='firmhash') \\\n",
    ".merge(df_contron_2, how='left', on='firmhash') \\\n",
    ".merge(df_employee, how='left', on='firmhash') \\\n",
    ".merge(df_iss_share_pros_count, how='left', on='firmhash') \\\n",
    ".merge(df_ig_manuals, how='left', on='firmhash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blended['legislation_complied_with'] = df_blended['legislation_complied_with'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blended.to_csv('../data/modelinput/information_governance_full_dataset.csv',\n",
    "                  index=False,\n",
    "                  quoting=csv.QUOTE_NONNUMERIC,\n",
    "                  quotechar='\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
