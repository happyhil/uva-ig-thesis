{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import PyPDF2\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from src import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_pdf(response):\n",
    "    \n",
    "    tmp_file = f'../data/policies/tmp/temp.pdf'\n",
    "    if os.path.isfile(tmp_file):\n",
    "        os.remove(tmp_file)\n",
    "    with open(tmp_file, 'wb') as pdf:\n",
    "        pdf.write(response.content)\n",
    "    with open(tmp_file, 'rb') as pdf:\n",
    "        pdfreader = PyPDF2.PdfFileReader(pdf)\n",
    "        pdftext = ''\n",
    "        for n in range(pdfreader.getNumPages()):\n",
    "            pdfpage = pdfreader.getPage(n)\n",
    "            pdftext += pdfpage.extractText()\n",
    "            pdftext += '\\n'\n",
    "    return pdftext\n",
    "\n",
    "def _conver_html(response):\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    htmlbody = soup.find('body')\n",
    "    for tag in htmlbody.find_all('a'):\n",
    "        tag.replaceWith('')\n",
    "    htmltext = htmlbody.get_text(strip=True)\n",
    "    return htmltext\n",
    "\n",
    "def scrape_webpage(url):\n",
    "    \n",
    "    headers = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9,en;q=0.8\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15\"\n",
    "      }\n",
    "    \n",
    "    status = False\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, stream=True, timeout=20)\n",
    "        time.sleep(3)\n",
    "        status = True\n",
    "    except Exception as e:\n",
    "        print('===> Problem occured during html request')\n",
    "        print(f'===> {e}')\n",
    "    if status:\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                contenttype = response.headers.get('content-type')\n",
    "                if 'application/pdf' in contenttype:\n",
    "                    text = _convert_pdf(response)\n",
    "                elif 'text/html' in contenttype:\n",
    "                    text = _conver_html(response)\n",
    "                else:\n",
    "                    print(f'unknown content-type: {contenttype}')\n",
    "                    text = ''\n",
    "            except Exception as e:\n",
    "                print('===> Problem occured during response to text convertion')\n",
    "                print(e)\n",
    "                text = ''\n",
    "            return text, response.status_code\n",
    "        else:\n",
    "            print(f'===> Response: {response.status_code}')\n",
    "            return '', response.status_code\n",
    "    else:\n",
    "        return '', -999\n",
    "    \n",
    "def main(ppurls):\n",
    "    \n",
    "    logdict = {}\n",
    "    \n",
    "    for k, v in ppurls.items():\n",
    "        firmhash = common.__hash(k)\n",
    "        print(f'==> get request for: {k} ({firmhash}): {v[\"ppurl\"]}')\n",
    "        text, status_code = scrape_webpage(v['ppurl'])\n",
    "        with open(f'../data/policies/scraped/{firmhash}_privacy_policy.txt', 'w') as outfile:\n",
    "            outfile.write(text)\n",
    "            \n",
    "        logdict[firmhash] = {\n",
    "            'firm': k,\n",
    "            'ppurl': v['ppurl'],\n",
    "            'n_char': len(text),\n",
    "            'statuscode':  status_code\n",
    "        }\n",
    "    \n",
    "    now = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    with open('../data/policies/tmp/policies_scrape_log.json', 'w') as logstream:\n",
    "        json.dump(logdict, logstream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-rabbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('../data/policies/urls/privacy_policy_urls_corrected.json'):\n",
    "    with open('../data/policies/urls/privacy_policy_urls_corrected.json', 'r') as infile:\n",
    "        ppurls = json.load(infile)\n",
    "else:\n",
    "    with open('../data/policies/urls/privacy_policy_urls_20210316.json', 'r') as infile:\n",
    "        ppurls = json.load(infile)\n",
    "main(ppurls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
