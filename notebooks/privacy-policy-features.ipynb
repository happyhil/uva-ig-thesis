{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "floating-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import textstat\n",
    "import spacy\n",
    "from src import common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-vault",
   "metadata": {},
   "source": [
    "## -- define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spectacular-turtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../resources/easywords.txt', 'r') as easyfile:\n",
    "    easywords = []\n",
    "    for line in easyfile:\n",
    "        easywords.append(line.strip())\n",
    "\n",
    "with open('../resources/ambiguouswords.txt', 'r') as ambifile:\n",
    "    ambiwords = []\n",
    "    for line in ambifile:\n",
    "        ambiwords.append(line.strip())\n",
    "\n",
    "if os.path.isfile('../data/policies/urls/privacy_policy_urls_corrected.json'):\n",
    "    with open('../data/policies/urls/privacy_policy_urls_corrected.json', 'r') as infile:\n",
    "        ppurls = json.load(infile)\n",
    "else:\n",
    "    with open('../data/policies/urls/privacy_policy_urls_20210316.json', 'r') as infile:\n",
    "        ppurls = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "empirical-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sentences(text):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    \n",
    "    document_sentences_tokens = []\n",
    "    for s in doc.sents:\n",
    "        sentence_tokens = []\n",
    "        for t in s:\n",
    "            token = t.text.translate(translator).lower()\n",
    "            token_clean = re.sub(\"[^0-9a-zA-Z]+\", \"\", token)\n",
    "            if 0 < len(token_clean) < 25:\n",
    "                sentence_tokens.append(token_clean)\n",
    "        if len(sentence_tokens) > 0:\n",
    "            document_sentences_tokens.append(sentence_tokens)\n",
    "    \n",
    "    return document_sentences_tokens\n",
    "\n",
    "def convert_to_tokens(sentences):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    tokens = [str(token) for s in sentences for token in s]\n",
    "\n",
    "    difficult_tokens = []\n",
    "    for t in tokens:\n",
    "        syllable_count = count_token_syllables(t)\n",
    "        if t not in easywords and syllable_count >= 2:\n",
    "            difficult_tokens.append(t)\n",
    "            \n",
    "    return tokens, difficult_tokens\n",
    "\n",
    "def count_token_syllables(word):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    return textstat.syllable_count(word)\n",
    "\n",
    "def statistics(sentences):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    tokens, _ = convert_to_tokens(sentences)\n",
    "    \n",
    "    return {\n",
    "        'n_sentences': len(sentences),\n",
    "        'n_tokens': len(tokens),\n",
    "        'n_unique_tokens': len(set(tokens))\n",
    "    }\n",
    "\n",
    "def gunning_fog_index(sentences):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    tokens, difficult_tokens = convert_to_tokens(sentences)\n",
    "    n_words = len(tokens)\n",
    "    n_difficult_words = len(difficult_tokens)\n",
    "    n_sentences = len(sentences)\n",
    "    \n",
    "    return round(0.4 * ((n_words / n_sentences) + 100 * (n_difficult_words / n_words)), 10)\n",
    "\n",
    "def ambiquity_score(sentences, ambiwords):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    tokens, _ = convert_to_tokens(sentences)\n",
    "    \n",
    "    ambi_tokens_count = len([x for x in tokens if x in ambiwords])\n",
    "    ambi_score = ambi_tokens_count / len(tokens)\n",
    "    \n",
    "    return round(ambi_score, 10)\n",
    "\n",
    "def run_text_analysis(text):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    sentences = convert_to_sentences(policytext)\n",
    "    results = statistics(sentences)\n",
    "    results['gunning_fog'] = gunning_fog_index(sentences)\n",
    "    results['ambiquity_score'] = ambiquity_score(sentences, ambiwords)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-support",
   "metadata": {},
   "source": [
    "## -- execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-charlotte",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ally Financial ==> ZeroDivisionError: division by zero\n",
      "Lithia Motors ==> ZeroDivisionError: division by zero\n",
      "L Brands ==> ZeroDivisionError: division by zero\n",
      "Ameriprise Financial ==> ZeroDivisionError: division by zero\n",
      "Lincoln National ==> ZeroDivisionError: division by zero\n",
      "EOG Resources ==> ZeroDivisionError: division by zero\n",
      "Danaher ==> ZeroDivisionError: division by zero\n",
      "Broadcom ==> ZeroDivisionError: division by zero\n",
      "Dollar Tree ==> ZeroDivisionError: division by zero\n",
      "Enterprise Products Partners ==> ZeroDivisionError: division by zero\n",
      "Energy Transfer ==> ZeroDivisionError: division by zero\n"
     ]
    }
   ],
   "source": [
    "textstat.set_lang('en')\n",
    "\n",
    "full_text_features = {}\n",
    "\n",
    "verbose = False\n",
    "count = 0\n",
    "total_success = 0\n",
    "total_error = 0\n",
    "for k, v in ppurls.items():\n",
    "    count += 1\n",
    "    \n",
    "    firmhash = common.__hash(k)\n",
    "    if os.path.isfile(f'../data/policies/manual/{firmhash}_privacy_policy.txt'):\n",
    "        with open(f'../data/policies/manual/{firmhash}_privacy_policy.txt', 'r') as infile:\n",
    "            policytext = infile.read()\n",
    "            go = True\n",
    "    elif os.path.isfile(f'../data/policies/scraped/{firmhash}_privacy_policy.txt'):\n",
    "        with open(f'../data/policies/scraped/{firmhash}_privacy_policy.txt', 'r') as infile:\n",
    "            policytext = infile.read()\n",
    "            go = True\n",
    "    else:\n",
    "        go = False\n",
    "    \n",
    "    if go:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            result = run_text_analysis(policytext)\n",
    "\n",
    "            full_text_features[firmhash] = {\n",
    "                'firm': k,\n",
    "                'ppurl': v['ppurl'],\n",
    "                'succeeded': True,\n",
    "                'features': result\n",
    "            }\n",
    "            \n",
    "            total_success += 1\n",
    "            if verbose:\n",
    "                print(f'{k} ==> done')\n",
    "                print(f' - Number of sentences: {result[\"n_unique_tokens\"]}')\n",
    "                print(f' - Number of words: {result[\"n_tokens\"]}')\n",
    "                print(f' - Number of unique words: {result[\"n_unique_tokens\"]}')\n",
    "                print(f' - Ambiguous words: {round(result[\"ambiquity_score\"] *100, 3)}%')\n",
    "                print(f' - Fog readability score: {result[\"gunning_fog\"]}')\n",
    "            \n",
    "        except ValueError as err:\n",
    "            if verbose:\n",
    "                print(f'{k} ==> ValueError: {err}')\n",
    "            total_error += 1\n",
    "            full_text_features[firmhash] = {\n",
    "                'firm': k,\n",
    "                'ppurl': v['ppurl'],\n",
    "                'succeeded': False,\n",
    "                'error': 'ValueError'\n",
    "            }\n",
    "            \n",
    "        except ZeroDivisionError as err:\n",
    "            if verbose:\n",
    "                print(f'{k} ==> ZeroDivisionError: {err}')\n",
    "            total_error += 1\n",
    "            full_text_features[firmhash] = {\n",
    "                'firm': k,\n",
    "                'ppurl': v['ppurl'],\n",
    "                'succeeded': False,\n",
    "                'error': 'ZeroDivisionError'\n",
    "            }\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        full_text_features[firmhash] = {\n",
    "                'firm': k,\n",
    "                'ppurl': v['ppurl'],\n",
    "                'succeeded': False,\n",
    "                'error': 'NoFile'\n",
    "            }\n",
    "            \n",
    "\n",
    "    with open('../data/policies/features/firm_pp_features_0.1.0.json', 'w') as outfile:\n",
    "        json.dump(full_text_features, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total successes: {total_success}')\n",
    "print(f'Total errors: {total_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-tiffany",
   "metadata": {},
   "source": [
    "## -- checks + to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "full_text_features_checked = {k: v for k, v in full_text_features.items() if v['succeeded']}\n",
    "\n",
    "df_features = pd.DataFrame({\n",
    "    'firmhash': [k for k in full_text_features_checked.keys()],\n",
    "    'firm': [v['firm'] for v in full_text_features_checked.values()],\n",
    "    'n_sentence': [v['features']['n_sentences'] for v in full_text_features_checked.values()],\n",
    "    'number_of_words': [v['features']['n_tokens'] for v in full_text_features_checked.values()],\n",
    "    'number_of_unique_words': [v['features']['n_unique_tokens'] for v in full_text_features_checked.values()],\n",
    "    'ambiquity_score': [v['features']['ambiquity_score'] for v in full_text_features_checked.values()],\n",
    "    'gunning_fog_index': [v['features']['gunning_fog'] for v in full_text_features_checked.values()]\n",
    "})\n",
    "\n",
    "df_features.to_csv('../data/policies/features/firm_pp_features_0.2.0.csv',\n",
    "                   index=False,\n",
    "                   quotechar='\"',\n",
    "                   quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-telescope",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in full_text_features.items() if not v['succeeded']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in full_text_features_checked.items() if v['features']['n_tokens'] < 400}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-sailing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
