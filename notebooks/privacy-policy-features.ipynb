{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import string\n",
    "import textstat\n",
    "# from readability import Readability\n",
    "from src import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check manual folder, if not then check scraper folder\n",
    "# TODO: remove dubble scape in cleaning  \n",
    "# TODO: implement tries\n",
    "# TODO: make custom fog index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-criterion",
   "metadata": {},
   "source": [
    "## -- define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMBI_WORDS = ['Occasional', 'will', 'perhaps', 'such', 'some', 'certain', 'various', 'reasonable', 'like', 'example', 'sometimes', 'depending', 'necessary', 'appropriate', 'inappropriate',\n",
    "'generally', 'mostly','widely', 'general', 'commonly', 'usually', 'normally', 'typically', 'largely', 'often', 'may', 'might', 'can', 'could', 'would', 'likely', 'possible', 'possibly',\n",
    "'unsure', 'anyone', 'certain', 'everyone', 'numerous', 'some', 'most', 'few', 'much', 'many', 'various']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('../data/policies/urls/privacy_policy_urls_corrected.json'):\n",
    "    with open('../data/policies/urls/privacy_policy_urls_corrected.json', 'r') as infile:\n",
    "        ppurls = json.load(infile)\n",
    "else:\n",
    "    with open('../data/policies/urls/privacy_policy_urls_20210316.json', 'r') as infile:\n",
    "        ppurls = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    text_lowered = text.lower()\n",
    "    text_cleaned = text_lowered.translate(translator)\n",
    "    \n",
    "    return text_cleaned\n",
    "\n",
    "def score_readability(text, score_methods):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "#     r = Readability(text)\n",
    "    \n",
    "    scores = {}\n",
    "    for m in score_methods:\n",
    "#         scores[m] = r.smog()\n",
    "        scores[m] = textstat.gunning_fog(text)\n",
    "        \n",
    "    \n",
    "    return scores\n",
    "\n",
    "def score_tokens(text):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    tokens = [t for t in text.split(' ') if len(t) > 1]\n",
    "    unique_tokens = set(tokens)\n",
    "    ambi_tokens_count = len([x for x in tokens if x in AMBI_WORDS])\n",
    "    score = ambi_tokens_count / len(tokens)\n",
    "    \n",
    "    return {\n",
    "        'n_tokens': len(tokens),\n",
    "        'n_unique_tokens': len(unique_tokens),\n",
    "        'ambiquity_score': score\n",
    "    }\n",
    "\n",
    "def run_text_analysis(text, score_methods):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    text_clean = clean_text(text)    \n",
    "    \n",
    "    token_result = score_tokens(text_clean)\n",
    "    readability_result = score_readability(text, score_methods)\n",
    "    \n",
    "    result = {**token_result, **readability_result}\n",
    "    result['n_sentence'] = textstat.sentence_count(text)\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-suspension",
   "metadata": {},
   "source": [
    "## -- execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "textstat.set_lang('en')\n",
    "\n",
    "full_text_features = {}\n",
    "\n",
    "verbose = False\n",
    "count = 0\n",
    "total_success = 0\n",
    "total_error = 0\n",
    "for k, v in ppurls.items():\n",
    "    count += 1\n",
    "    \n",
    "    firmhash = common.__hash(k)\n",
    "    if os.path.isfile(f'../data/policies/scraped/{firmhash}_privacy_policy.txt'):\n",
    "        with open(f'../data/policies/scraped/{firmhash}_privacy_policy.txt', 'r') as infile:\n",
    "            policytext = infile.read()\n",
    "            go = True\n",
    "    \n",
    "    else:\n",
    "        go = False\n",
    "    \n",
    "    if go:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            result = run_text_analysis(policytext, score_methods=['gunning_fog'])\n",
    "\n",
    "            full_text_features[firmhash] = {\n",
    "                'firm': k,\n",
    "                'ppurl': v['ppurl'],\n",
    "                'features': result\n",
    "            }\n",
    "            \n",
    "            total_success += 1\n",
    "            \n",
    "            print(f'{k} ==> done')\n",
    "            if verbose:\n",
    "                print(f' - Number of words: {result[\"n_tokens\"]}')\n",
    "                print(f' - Number of unique words: {result[\"n_unique_tokens\"]}')\n",
    "                print(f' - Ambiguous words: {round(result[\"ambiquity_score\"] *100, 3)}%')\n",
    "                print(f' - Number of sentences: {result[\"n_unique_tokens\"]}')\n",
    "                print(f' - Fog readability score: {result[\"gunning_fog\"]}')\n",
    "            \n",
    "        except ValueError as err:\n",
    "            print(f'{k} ==> ValueError: {err}')\n",
    "            total_error += 1\n",
    "            \n",
    "        except ZeroDivisionError as err:\n",
    "            print(f'{k} ==> ZeroDivisionError: {err}')\n",
    "            total_error += 1\n",
    "\n",
    "    with open('../data/policies/features/firm_pp_features_0.1.0.json', 'w') as outfile:\n",
    "        json.dump(full_text_features, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total successes: {total_success}')\n",
    "print(f'Total errors: {total_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-samoa",
   "metadata": {},
   "source": [
    "## -- checks and to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-trustee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.DataFrame({\n",
    "    'firmhash': [k for k in full_text_features.keys()],\n",
    "    'firm': [v['firm'] for v in full_text_features.values()],\n",
    "    'number_of_words': [v['features']['n_tokens'] for v in full_text_features.values()],\n",
    "    'number_of_unique_words': [v['features']['n_unique_tokens'] for v in full_text_features.values()],\n",
    "    'n_sentence': [v['features']['n_sentence'] for v in full_text_features.values()],\n",
    "    'ambiquity_score': [v['features']['ambiquity_score'] for v in full_text_features.values()],\n",
    "    'gunning_fog_score': [v['features']['gunning_fog'] for v in full_text_features.values()]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-consent",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.to_csv('../data/policies/features/firm_pp_features_0.1.0.csv', index=False, quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
